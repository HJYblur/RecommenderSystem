{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f583fc73-473b-423f-aee7-376077a2321a",
   "metadata": {},
   "source": [
    "# DSAIT4335 Recommender Systems\n",
    "# Assignment 1: Content-based Recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d518530c-38b5-4e5e-bbcc-1dcba666ef94",
   "metadata": {},
   "source": [
    "In this assignment, you will work to build a content-based recommendation model using different text processing methods. Then, you will apply your content-based recommendation model on a public dataset. The dataset is **MovieLens100K**, a movie recommendation dataset collected by GroupLens: https://grouplens.org/datasets/movielens/100k/.\n",
    "\n",
    "By the end of this assignment, you will:\n",
    "1. Understand the fundamental principles of content-based recommender systems\n",
    "2. Develop feature extraction method (BERT)\n",
    "3. Build user and item profiles from content features\n",
    "4. Perform both rating prediction and top-k recommendation tasks\n",
    "5. Evaluate content-based methods to understand their strengths/limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5473ebe1-c51b-485a-90f5-beefb1c8e9b9",
   "metadata": {},
   "source": [
    "# Instruction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6470aee6-0d13-496f-883a-f224469b4df3",
   "metadata": {},
   "source": [
    "The MovieLens100K is already splitted into 80% training and 20% test sets. Along with training and test sets, movies metadata as content information is also provided.\n",
    "\n",
    "**Expected file structure** for this assignment:   \n",
    "   \n",
    "   ```\n",
    "   Assignment1/\n",
    "   ├── training.txt\n",
    "   ├── test.txt\n",
    "   ├── movies.txt\n",
    "   └── hw1.ipynb\n",
    "   ```\n",
    "\n",
    "**Note:** Be sure to run all cells in each section sequentially, so that intermediate variables and packages are properly carried over to subsequent cells.\n",
    "\n",
    "**Submission:** Answer all the questions in this jupyter-notebook file. Submit this jupyter-notebook file (your answers included) to Brightspace. Change the name of this jupyter-notebook file to your name: firstname-lastname.ipynb."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028371e8-3f20-456f-bc81-c2b1cead2f13",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20bf874a-285c-4f84-8d63-23f16b2289bb",
   "metadata": {},
   "source": [
    "Import necessary libraries/packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfb2c8cb-4471-4fb2-9da5-5204db2ff286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch-mutex             1.0                        cuda    pytorch-nightly\n",
      "sentence-transformers     3.3.1              pyhd8ed1ab_0    conda-forge\n",
      "torch                     2.6.0                    pypi_0    pypi\n",
      "torch-tb-profiler         0.4.3                    pypi_0    pypi\n",
      "torchvision               0.21.0                   pypi_0    pypi\n",
      "transformers              4.51.0.dev0              pypi_0    pypi\n"
     ]
    }
   ],
   "source": [
    "# !pip install transformers torch  # For BERT\n",
    "\n",
    "# you can refer https://huggingface.co/docs/transformers/en/model_doc/bert for various versions of the pre-trained model BERT\n",
    "\n",
    "# Check if transformers & torch are in conda list\n",
    "!conda list | grep -E 'transformers|torch'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "750a1aca-9784-4dc3-9ad3-d4d4fa5a5e3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check the status of BERT installation:\n",
      "BERT libraries loaded successfully!\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# For BERT embeddings (install: pip install transformers torch)\n",
    "print(\"Check the status of BERT installation:\")\n",
    "\n",
    "try:\n",
    "    from transformers import AutoTokenizer, AutoModel\n",
    "    import torch\n",
    "    BERT_AVAILABLE = True\n",
    "    print(\"BERT libraries loaded successfully!\")\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "except ImportError:\n",
    "    BERT_AVAILABLE = False\n",
    "    print(\"BERT libraries not available. Install with: pip install transformers torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4634c0de-06eb-44d1-979b-8c0716f5e604",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "from sklearn.preprocessing import StandardScaler, MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Basic libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fa15d0-58b0-4d34-8659-aeb64c110510",
   "metadata": {},
   "source": [
    "# 1) MovieLens100K dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64fab09-20a4-4a21-a27a-d459a2b2b2b1",
   "metadata": {},
   "source": [
    "Before building content-based recommender, we need to do exploratory data analysis to thoroughly understand our data and its content features.\n",
    "\n",
    "Preliminary: Take a glimpse of the following dataframes to check what the training, test, and movie data files constitute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a07dcae-eec6-4660-bc57-e2b5dcc258a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the training set and test set\n",
    "columns_name=['user_id','item_id','rating','timestamp']\n",
    "train_data = pd.read_csv('training.txt', sep='\\t', names=columns_name)\n",
    "test_data = pd.read_csv('test.txt', sep='\\t', names=columns_name)\n",
    "\n",
    "print(f'The training data:')\n",
    "display(train_data[['user_id','item_id','rating']].head())\n",
    "print(f'The shape of the training data: {train_data.shape}')\n",
    "print('--------------------------------')\n",
    "print(f'The test data:')\n",
    "display(test_data[['user_id','item_id','rating']].head())\n",
    "print(f'The shape of the test data: {test_data.shape}')\n",
    "# print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9749557-7633-4f71-adc2-97f5aaf43129",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = pd.read_csv('movies.txt',names=['item_id','title','genres','description'],sep='\\t')\n",
    "movies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43848c28-e024-4985-90ed-14e8f1c3517d",
   "metadata": {},
   "source": [
    "### Question 1: How many users, items, and ratings are in the training set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2769776-e205-497e-88cf-91baec2008a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_stats(train_data):\n",
    "    \"\"\"\n",
    "    Perform basic statistical analysis of the MovieLens100K dataset.\n",
    "    \"\"\"\n",
    "    if train_data is None:\n",
    "        print(\"Please load the MovieLens dataset first!\")\n",
    "        return\n",
    "    \n",
    "    n_users = 0\n",
    "    n_movies = 0\n",
    "    n_ratings = 0\n",
    "    \n",
    "    ############# Your code here ############\n",
    "    \n",
    "    #########################################\n",
    "    \n",
    "    return n_users, n_movies, n_ratings\n",
    "\n",
    "n_users, n_movies, n_ratings = get_data_stats(train_data)\n",
    "print(\"Dataset Analysis\")\n",
    "print(\"=\" * 30)\n",
    "print(f\"Number of Users: {n_users:,}\")\n",
    "print(f\"Number of Movies: {n_movies:,}\")\n",
    "print(f\"Number of Actual Ratings: {n_ratings:,}\")\n",
    "print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9b55a4-b690-451f-b41d-f4dc540e6063",
   "metadata": {},
   "source": [
    "### Question 2: What is the sparsity of the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9086ecaf-393a-4fbb-851c-92e963cb951e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_UIM_sparsity(train_data):\n",
    "    # Implement the function that returns the fraction of missing data in user-item rating matrix. \n",
    "    # You can call get_data_stats(train_data) function for getting the necessary variables.\n",
    "    \n",
    "    sparsity = 0.0\n",
    "\n",
    "    ############# Your code here ############\n",
    "\n",
    "    #########################################\n",
    "\n",
    "    return sparsity\n",
    "\n",
    "sparsity = get_UIM_sparsity(train_data)\n",
    "print(\"Sparsity of the data is {}\".format(sparsity))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0144366d-24b8-4b33-9d36-de82d288d943",
   "metadata": {},
   "source": [
    "### Question 3: Create the histogram of movie title length. Set the number of bins to 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45873f1-ebec-4d8c-93a2-b602199ba56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hist_title_length(movies):\n",
    "    # Given movies dataframe, implement the function that generates a histogram of movies title length. \n",
    "    # Hint: in histogram, x-axis shows the length of title, and y-axis shows the number of movies with the corresponding length.\n",
    "\n",
    "    ############# Your code here ############\n",
    "    \n",
    "    #########################################\n",
    "    \n",
    "hist_title_length(movies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61e063e-3618-42b6-b3dc-75b94d9b21b8",
   "metadata": {},
   "source": [
    "Discuss your observations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c530d8f-0f01-4604-8532-bdbc64efcbf3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ee09b075-0a2a-4996-92c1-f6bd14370fb0",
   "metadata": {},
   "source": [
    "### Question 4: Create the histogram of movie description length. Set the number of bins to 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e43adcf-e7e2-4986-b0a6-949f7691e18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hist_description_length(movies):\n",
    "    # Given movies dataframe, implement the function that generates a histogram of movies description length. \n",
    "    # Hint: in histogram, x-axis shows the length of description, and y-axis shows the number of movies with the corresponding length.\n",
    "\n",
    "    ############# Your code here ############\n",
    "    \n",
    "    #########################################\n",
    "    \n",
    "hist_description_length(movies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4aa85f6-d73c-463f-a1c0-e402e353653d",
   "metadata": {},
   "source": [
    "Discuss your observations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b89f3c-f785-4a36-af97-5e5d778e04f0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dcb09efc-53b5-4bd7-b39c-3d45689b7a9a",
   "metadata": {},
   "source": [
    "# 2) Deriving content representation with BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55fbe86f-8abe-4b86-a9db-86ff15141e31",
   "metadata": {},
   "source": [
    "BERT (Bidirectional Encoder Representations from Transformers) provides rich, contextual embeddings that can capture semantic meaning. See [Devlin, Jacob, et al. \"Bert: Pre-training of deep bidirectional transformers for language understanding.\" NAACL-HLT 2 2019](https://aclanthology.org/N19-1423.pdf) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ee12d81-ab0a-49d4-8dda-99b90a1d0c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bert_embeddings(content):\n",
    "    \"\"\"\n",
    "    Generate BERT embeddings for movie content.\n",
    "\n",
    "    Args:\n",
    "        content: Content of items\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: BERT embeddings matrix\n",
    "    \"\"\"\n",
    "    if not BERT_AVAILABLE:\n",
    "        print(\"BERT libraries not available. Install with: pip install transformers torch\")\n",
    "        return None\n",
    "\n",
    "    if content is None:\n",
    "        return None\n",
    "\n",
    "    if isinstance(content, pd.Series):\n",
    "        content = content.fillna(\"\").astype(str).tolist()\n",
    "    elif isinstance(content, np.ndarray):\n",
    "        content = content.astype(str).tolist()\n",
    "\n",
    "    model_name = 'distilbert-base-uncased'\n",
    "\n",
    "    print(f\"Loading BERT model: {model_name}\")\n",
    "\n",
    "    # Load tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "    # Set device (GPU if available)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using cuda or cpu: {device}\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Generate embeddings in batches\n",
    "    batch_size = 32  # Adjust based on available memory\n",
    "    emb = []\n",
    "\n",
    "    for i in range(0, len(content), batch_size):\n",
    "        if i % (batch_size * 10) == 0:\n",
    "            print(f\"Processing batch {i//batch_size + 1}/{len(content)//batch_size + 1}\")\n",
    "\n",
    "        batch_texts = content[i:i + batch_size]\n",
    "\n",
    "        # Tokenize batch\n",
    "        inputs = tokenizer(\n",
    "            batch_texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        # Move to device\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "        # Generate embeddings\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "            # Use [CLS] token embedding (first token)\n",
    "            batch_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "            emb.extend(batch_embeddings)\n",
    "\n",
    "    emb = np.array(emb)\n",
    "\n",
    "    print(f\"BERT embeddings generated: {emb.shape}\")\n",
    "    print(f\"Embedding dimension: {emb.shape[1]}\")\n",
    "\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d85bf4-2c7e-41d7-b553-bd9859b0f141",
   "metadata": {},
   "source": [
    "You can get the content representation using BERT as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161a54c1-1a68-4165-b4c7-0beacef3992a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample content\n",
    "sample_content = ['aa','ab','ac','bc']\n",
    "\n",
    "# Generate BERT embeddings (this may take several minutes)\n",
    "print(\"Generating BERT embeddings...\")\n",
    "bert_embeddings = create_bert_embeddings(sample_content)\n",
    "\n",
    "if bert_embeddings is not None:\n",
    "    print(\"BERT embeddings created successfully!\")\n",
    "else:\n",
    "    print(\"BERT embeddings not available. Continuing with TF-IDF only.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549328d7-e3f6-42ad-be05-ccaadeb4c31d",
   "metadata": {},
   "source": [
    "### Question 5: Derive the representation of items for three types of content: \n",
    "    1) title + genres \n",
    "    2) description \n",
    "    3) title + genres + description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feaafe9b-5b5f-42bd-a092-8bb66abbed34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement code to derive the content representation for title and genres. Concatenate the two content as: title + ' ' + genres\n",
    "item_emb_titlegenres = None\n",
    "############# Your code here ############\n",
    "\n",
    "#########################################\n",
    "\n",
    "# Implement code to derive the content representation for description.\n",
    "item_emb_description = None\n",
    "############# Your code here ############\n",
    "\n",
    "#########################################\n",
    "\n",
    "# Implement code to derive the content representation for title, genres, and description. Concatenate the two content as: title + ' ' + genres + '' + description\n",
    "item_emb_full = None\n",
    "############# Your code here ############\n",
    "\n",
    "#########################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3184e68a-0a7c-4742-8192-454ca1202f04",
   "metadata": {},
   "source": [
    "### Question 6: What is the embedding of item_id=100?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce531cf-be9e-4e31-a31c-19b1c436d797",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_item_emb(item_id, content_type):\n",
    "    # Implement the function that given content type (title+genres, description, or title+genres+description) returns the embedding derived for the corresponding item_id. \n",
    "    # Hint1: keep in mind that item_id in the data starts from 1, but in the embedding variable it starts from 0, e.g., item_id 100 corresponds to index 99 in embedding variable..\n",
    "    # Hint2: use if-else conditions to return the embedding for the requested content types.\n",
    "    # Hint3: use the global variables (embeddings) already computed in previous cells.\n",
    "\n",
    "    emb = None\n",
    "    \n",
    "    ############# Your code here ############\n",
    "    \n",
    "    #########################################\n",
    "\n",
    "    return emb\n",
    "    \n",
    "item_id = 100\n",
    "print('Embedding representation for content type = title+genres:')\n",
    "print(get_item_emb(item_id, 'title_genres'))\n",
    "print('------------------------------------------')\n",
    "print('Embedding representation for content type = description:')\n",
    "print(get_item_emb(item_id, 'description'))\n",
    "print('------------------------------------------')\n",
    "print('Embedding representation for content type = full:')\n",
    "print(get_item_emb(item_id, 'full'))\n",
    "print('------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad6b63a-a947-4403-86fd-ece292279e29",
   "metadata": {},
   "source": [
    "# 3) User profile construction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1215fb-3c99-426d-b281-1d51cf8a5842",
   "metadata": {},
   "source": [
    "### Question 7: What are the embeddings and ratings of interacted items by user_id=100?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbb7c14-8be8-4269-8dd3-ec46008c8cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_interacted_items_embs_rating(train_data, user_id, content_type):\n",
    "    # Implement the function that given content type (title+genres, description, or title+genres+description) returns the embeddings and ratings of interacted items by user_id=100. \n",
    "    # Hint1: use train_data to retrieve the item_ids that target user (user_id=100 in this example) interacted, then pass these item_ids to function previously implemented to retrieve the embeddings and ratings.\n",
    "\n",
    "    embs, ratings = [], []\n",
    "    \n",
    "    ############# Your code here ############\n",
    "    \n",
    "    #########################################\n",
    "\n",
    "    return embs, ratings\n",
    "    \n",
    "user_id = 100\n",
    "print('Embeddings and ratings of interacted items by user_id=100 for content type = title+genres:')\n",
    "print(get_interacted_items_embs_rating(train_data, user_id, 'title_genres'))\n",
    "print('------------------------------------------')\n",
    "print('Embeddings and ratings of interacted items by user_id=100 for content type = description:')\n",
    "print(get_interacted_items_embs_rating(train_data, user_id, 'description'))\n",
    "print('------------------------------------------')\n",
    "print('Embeddings and ratings of interacted items by user_id=100 for content type = full:')\n",
    "print(get_interacted_items_embs_rating(train_data, user_id, 'full'))\n",
    "print('------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6caf5f94-9d6c-46c8-94c4-26371cdb68ff",
   "metadata": {},
   "source": [
    "### Question 8: Derive the representation for user_id=100 using the following aggregation methods:\n",
    "1. **avg:** Average representation of interacted item \n",
    "2. **weighted_avg:** Weighted average representation of interacted item using rating values\n",
    "3. **avg_pos:** Average representation of positively interacted item (ratings >= 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61c6057-422d-4205-bec1-99d3e8c8b424",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_user_emb(train_data, user_id, content_type, aggregation_method):\n",
    "    # Implement the function that given content type (title+genres, description, or title+genres+description) and aggregation method (avg, weighted_avg, avg_pos) returns the representation of a user. \n",
    "    # Hint1: use the previsouly implemented items for retrieving ratings and representation of interacted items by a user.\n",
    "\n",
    "    emb = []\n",
    "    \n",
    "    ############# Your code here ############\n",
    "    \n",
    "    #########################################\n",
    "\n",
    "    return emb\n",
    "    \n",
    "user_id = 100\n",
    "content_type, aggregation_method = 'full', 'avg' # alternatives are content_type={title_genres,description,full} and aggregation_method={avg,weighted_avg,avg_pos}\n",
    "print('Embeddings of user_id=100 for content type '+content_type+' by aggregation method '+aggregation_method+':')\n",
    "print(get_user_emb(train_data, user_id, content_type, aggregation_method))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e289bdb5-629c-4ce3-8c64-11d3e088feba",
   "metadata": {},
   "source": [
    "# 4) Content-based recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aead6959-4472-42ee-8889-cac82fd22c05",
   "metadata": {},
   "source": [
    "Predict the rating for a user-item pair. Use dot product between user and item ebmeddings to predict the score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d258ad66-e74d-434d-87e9-8353d639a05f",
   "metadata": {},
   "source": [
    "### Question 9: What is the predicted score for user_id=100 and item_id=266?\n",
    "\n",
    "**Note:** The predicted score might not be in the interval [1,5]. In the next part, after predicting the ratings for all user-item pair, the predictions will be normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6cb497-0f78-42bc-90ff-8c87e6077bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_item_prediction(train_data, user_id, item_id, content_type, aggregation_method):\n",
    "    # Implement the function that given content type and aggregation method returns the predicted rating for a user-item pair. \n",
    "    # Hint1: use the previsouly implemented functions for retrieving the embeddings and then compute the dot product of user and item embeddings.\n",
    "\n",
    "    pred_rating = 0.0\n",
    "    \n",
    "    ############# Your code here ############\n",
    "    \n",
    "    #########################################\n",
    "\n",
    "    return pred_rating\n",
    "    \n",
    "user_id, item_id = 100, 266\n",
    "content_type, aggregation_method = 'full', 'avg' # alternatives are content_type={title_genres,description,full} and aggregation_method={avg,weighted_avg,avg_pos}\n",
    "print('Predicted score for user_id=100 and item_id=266 for content type '+content_type+' and aggregation method '+aggregation_method+':')\n",
    "print(get_user_item_prediction(train_data, user_id, item_id, content_type, aggregation_method))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9b498d-f023-4c27-972d-141e91e35a3c",
   "metadata": {},
   "source": [
    "# 5) Metrics\n",
    "\n",
    "For this part, refer to the lecture on \"Evaluation of Recommender Systems\" where different metrics are described."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f3d080-c71f-46dc-ad81-e2cd7c4df1a8",
   "metadata": {},
   "source": [
    "### Question 10: Implement MAE, MSE, and RMSE for rating prediction task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b54fa32a-1c5c-4731-9769-a42bb816e7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MAE(actual_ratings, pred_ratings):\n",
    "    # Implement a function that computes MAE error between actual ratings and predicted ratings. \n",
    "    # Note that actual_ratings and pred_ratings are lists.\n",
    "    \n",
    "    result = 0.0\n",
    "    \n",
    "    ############# Your code here ############\n",
    "    \n",
    "    #########################################\n",
    "\n",
    "    return result\n",
    "\n",
    "def MSE(actual_rating, pred_rating):\n",
    "    # Implement a function that computes MSE error between actual ratings and predicted ratings. \n",
    "    # Note that actual_ratings and pred_ratings are lists.\n",
    "    \n",
    "    result = 0.0\n",
    "    \n",
    "    ############# Your code here ############\n",
    "    \n",
    "    #########################################\n",
    "\n",
    "    return result\n",
    "\n",
    "def RMSE(actual_rating, pred_rating):\n",
    "    # Implement a function that computes RMSE error between actual ratings and predicted ratings. \n",
    "    # Note that actual_ratings and pred_ratings are lists.\n",
    "    \n",
    "    result = 0.0\n",
    "    \n",
    "    ############# Your code here ############\n",
    "    \n",
    "    #########################################\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e64c4f-88d6-4807-8cab-067e6f2c8ac1",
   "metadata": {},
   "source": [
    "### Question 11: Implement Precision, Recall, NDCG, MRR, and MAP for ranking task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc91eb9-810c-4af7-ae40-b98435630cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Precision(ground_truth, rec_list):\n",
    "    # Implement a function that computes Precision across ground truth data and recommendation list generated for each user. \n",
    "    # Note that ground_truth and rec_list contain the list of items for all users, e.g., 2-dimensional arrays.\n",
    "    \n",
    "    result = 0.0\n",
    "    \n",
    "    ############# Your code here ############\n",
    "    \n",
    "    #########################################\n",
    "\n",
    "    return result\n",
    "\n",
    "def Recall(ground_truth, rec_list):\n",
    "    # Implement a function that computes Recall across ground truth data and recommendation list generated for each user. \n",
    "    # Note that ground_truth and rec_list contain the list of items for all users, e.g., 2-dimensional arrays.\n",
    "    \n",
    "    result = 0.0\n",
    "    \n",
    "    ############# Your code here ############\n",
    "    \n",
    "    #########################################\n",
    "\n",
    "    return result\n",
    "\n",
    "def NDCG(ground_truth, rec_list):\n",
    "    # Implement a function that computes NDCG across ground truth data and recommendation list generated for each user. \n",
    "    # Note that ground_truth and rec_list contain the list of items for all users, e.g., 2-dimensional arrays.\n",
    "    \n",
    "    result = 0.0\n",
    "    \n",
    "    ############# Your code here ############\n",
    "    \n",
    "    #########################################\n",
    "\n",
    "    return result\n",
    "\n",
    "def MRR(ground_truth, rec_list):\n",
    "    # Implement a function that computes MRR across ground truth data and recommendation list generated for each user. \n",
    "    # Note that ground_truth and rec_list contain the list of items for all users, e.g., 2-dimensional arrays.\n",
    "    \n",
    "    result = 0.0\n",
    "    \n",
    "    ############# Your code here ############\n",
    "    \n",
    "    #########################################\n",
    "\n",
    "    return result\n",
    "\n",
    "def MAP(ground_truth, rec_list):\n",
    "    # Implement a function that computes MAP across ground truth data and recommendation list generated for each user. \n",
    "    # Note that ground_truth and rec_list contain the list of items for all users, e.g., 2-dimensional arrays.\n",
    "    \n",
    "    result = 0.0\n",
    "    \n",
    "    ############# Your code here ############\n",
    "    \n",
    "    #########################################\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ad654b-c156-4c6e-8f89-ec10b1343d17",
   "metadata": {},
   "source": [
    "# 6) Evaluation of content-based recommender for rating prediction task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c7b09e-d6b2-4d30-af67-72b98946248d",
   "metadata": {},
   "source": [
    "### Question 12: Predict the ratings for all user-item pairs in test set and compute MAE, MSE, and RMSE. Discuss your observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c03f839-fea7-4ca8-886c-3f5e5af14abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_rating_prediction(train_data, test_data, content_type, aggregation_method):\n",
    "    # Implement a function that first computes the representation of users and then predicts the rating for each user-item pair. Finally, call the implemented metrics to measure the error.\n",
    "    # Hint: the reason for pre-computing the representation of all users is to speed up the experiments and to avoid too many unnecessary computations.\n",
    "    # Note: Make sure to map the predicted score into [1,5] interval. The prediction from content-based model may not necessarily be in 5-star rating scale.\n",
    "\n",
    "    # here we compute the representation of users\n",
    "    users_emb = []\n",
    "    users = list(train_data['user_id'].unique())\n",
    "    for user in tqdm(users):\n",
    "        users_emb.append(get_user_emb(train_data, user, content_type, aggregation_method))\n",
    "\n",
    "    print('Computing the representation of users is done!')\n",
    "\n",
    "    actual_ratings, pred_ratings = [], []\n",
    "    \n",
    "    ############# Your code here ############\n",
    "    \n",
    "    #########################################\n",
    "\n",
    "    # Given predicted ratings, map them into [1,5] interval using -> 1 + (pred - min_val) * (4 / (max_val - min_val))\n",
    "    ############# Your code here ############\n",
    "    \n",
    "    #########################################\n",
    "\n",
    "    mae_value, mse_value, rmse_value = 0.0, 0.0, 0.0\n",
    "\n",
    "    # compute the metrics: MAE, MSE, RMSE \n",
    "    ############# Your code here ############\n",
    "    \n",
    "    #########################################\n",
    "\n",
    "    return mae_value, mse_value, rmse_value\n",
    "\n",
    "print('Performance of content-based recommender for content type = title+genres and aggregation method = avg:')\n",
    "mae_value, mse_value, rmse_value = evaluate_rating_prediction(train_data, test_data, 'title_genres', 'avg')\n",
    "print('MAE='+str(round(mae_value,5))+', MSE='+str(round(mse_value,5))+', RMSE='+str(round(rmse_value,5)))\n",
    "print('------------------------------------------')\n",
    "print('Performance of content-based recommender for content type = description and aggregation method = avg:')\n",
    "mae_value, mse_value, rmse_value = evaluate_rating_prediction(train_data, test_data, 'description', 'avg')\n",
    "print('MAE='+str(round(mae_value,5))+', MSE='+str(round(mse_value,5))+', RMSE='+str(round(rmse_value,5)))\n",
    "print('------------------------------------------')\n",
    "print('Performance of content-based recommender for content type = full and aggregation method = avg:')\n",
    "mae_value, mse_value, rmse_value = evaluate_rating_prediction(train_data, test_data, 'full', 'avg')\n",
    "print('MAE='+str(round(mae_value,5))+', MSE='+str(round(mse_value,5))+', RMSE='+str(round(rmse_value,5)))\n",
    "print('------------------------------------------')\n",
    "print('Performance of content-based recommender for content type = title+genres and aggregation method = weighted_avg:')\n",
    "mae_value, mse_value, rmse_value = evaluate_rating_prediction(train_data, test_data, 'title_genres', 'weighted_avg')\n",
    "print('MAE='+str(round(mae_value,5))+', MSE='+str(round(mse_value,5))+', RMSE='+str(round(rmse_value,5)))\n",
    "print('------------------------------------------')\n",
    "print('Performance of content-based recommender for content type = description and aggregation method = weighted_avg:')\n",
    "mae_value, mse_value, rmse_value = evaluate_rating_prediction(train_data, test_data, 'description', 'weighted_avg')\n",
    "print('MAE='+str(round(mae_value,5))+', MSE='+str(round(mse_value,5))+', RMSE='+str(round(rmse_value,5)))\n",
    "print('------------------------------------------')\n",
    "print('Performance of content-based recommender for content type = full and aggregation method = weighted_avg:')\n",
    "mae_value, mse_value, rmse_value = evaluate_rating_prediction(train_data, test_data, 'full', 'weighted_avg')\n",
    "print('MAE='+str(round(mae_value,5))+', MSE='+str(round(mse_value,5))+', RMSE='+str(round(rmse_value,5)))\n",
    "print('------------------------------------------')\n",
    "print('Performance of content-based recommender for content type = title+genres and aggregation method = avg_pos:')\n",
    "mae_value, mse_value, rmse_value = evaluate_rating_prediction(train_data, test_data, 'title_genres', 'avg_pos')\n",
    "print('MAE='+str(round(mae_value,5))+', MSE='+str(round(mse_value,5))+', RMSE='+str(round(rmse_value,5)))\n",
    "print('------------------------------------------')\n",
    "print('Performance of content-based recommender for content type = description and aggregation method = avg_pos:')\n",
    "mae_value, mse_value, rmse_value = evaluate_rating_prediction(train_data, test_data, 'description', 'avg_pos')\n",
    "print('MAE='+str(round(mae_value,5))+', MSE='+str(round(mse_value,5))+', RMSE='+str(round(rmse_value,5)))\n",
    "print('------------------------------------------')\n",
    "print('Performance of content-based recommender for content type = full and aggregation method = avg_pos:')\n",
    "mae_value, mse_value, rmse_value = evaluate_rating_prediction(train_data, test_data, 'full', 'avg_pos')\n",
    "print('MAE='+str(round(mae_value,5))+', MSE='+str(round(mse_value,5))+', RMSE='+str(round(rmse_value,5)))\n",
    "print('------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af9c32d-0677-4cfd-a9de-b1b896d53f7e",
   "metadata": {},
   "source": [
    "Discuss your observations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357d7256-d743-4ddd-90c2-3ec32ea58763",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e019fcc3-cab4-415f-83af-d6fe508686ce",
   "metadata": {},
   "source": [
    "### Question 13: Generate recommendation list of size 10 for each user and compute Precision, Recall, NDCG, MRR, and MAP. Discuss your observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd3aaa0-6e9a-487e-86a5-159859d9a9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_rating_prediction(train_data, test_data, content_type, aggregation_method):\n",
    "    # Implement a function that first computes the representation of users and then predicts the relevance score of all items for each user. Next, return 10 unseen items with the highest predicted relevance score for each user as the recommendation list. Finally, call the implemented metrics to measure accuracy of recommendation.\n",
    "    # Hint: the reason for pre-computing the representation of all users is to speed up the experiments and to avoid too many unnecessary computations.\n",
    "    # Hint: no normalization is needed.\n",
    "\n",
    "    # here we compute the representation of users\n",
    "    users_emb = []\n",
    "    users = list(train_data['user_id'].unique())\n",
    "    for user in users:\n",
    "        users_emb.append(get_user_emb(train_data, user, content_type, aggregation_method))\n",
    "\n",
    "    ground_truth, rec_list = [], []\n",
    "    \n",
    "    ############# Your code here ############\n",
    "    \n",
    "    #########################################\n",
    "\n",
    "    precision_value, recall_value, ndcg_value, mrr_value, map_value = 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "\n",
    "    # compute the metrics: Precision, Recall, NDCG, MRR, MAP\n",
    "    ############# Your code here ############\n",
    "    \n",
    "    #########################################\n",
    "\n",
    "    return precision_value, recall_value, ndcg_value, mrr_value, map_value\n",
    "\n",
    "print('Performance of content-based recommender for content type = title+genres and aggregation method = avg:')\n",
    "precision_value, recall_value, ndcg_value, mrr_value, map_value = evaluate_rating_prediction(train_data, test_data, 'title_genres', 'avg')\n",
    "print('Precision='+str(round(precision_value,5))+', Recall='+str(round(recall_value,5))+', NDCG='+str(round(ndcg_value,5))+', MRR='+str(round(mrr_value,5))+', MAP='+str(round(map_value,5)))\n",
    "print('------------------------------------------')\n",
    "print('Performance of content-based recommender for content type = description and aggregation method = avg:')\n",
    "precision_value, recall_value, ndcg_value, mrr_value, map_value = evaluate_rating_prediction(train_data, test_data, 'description', 'avg')\n",
    "print('Precision='+str(round(precision_value,5))+', Recall='+str(round(recall_value,5))+', NDCG='+str(round(ndcg_value,5))+', MRR='+str(round(mrr_value,5))+', MAP='+str(round(map_value,5)))\n",
    "print('------------------------------------------')\n",
    "print('Performance of content-based recommender for content type = full and aggregation method = avg:')\n",
    "precision_value, recall_value, ndcg_value, mrr_value, map_value = evaluate_rating_prediction(train_data, test_data, 'full', 'avg')\n",
    "print('Precision='+str(round(precision_value,5))+', Recall='+str(round(recall_value,5))+', NDCG='+str(round(ndcg_value,5))+', MRR='+str(round(mrr_value,5))+', MAP='+str(round(map_value,5)))\n",
    "print('------------------------------------------')\n",
    "print('Performance of content-based recommender for content type = title+genres and aggregation method = weighted_avg:')\n",
    "precision_value, recall_value, ndcg_value, mrr_value, map_value = evaluate_rating_prediction(train_data, test_data, 'title_genres', 'weighted_avg')\n",
    "print('Precision='+str(round(precision_value,5))+', Recall='+str(round(recall_value,5))+', NDCG='+str(round(ndcg_value,5))+', MRR='+str(round(mrr_value,5))+', MAP='+str(round(map_value,5)))\n",
    "print('------------------------------------------')\n",
    "print('Performance of content-based recommender for content type = description and aggregation method = weighted_avg:')\n",
    "precision_value, recall_value, ndcg_value, mrr_value, map_value = evaluate_rating_prediction(train_data, test_data, 'description', 'weighted_avg')\n",
    "print('Precision='+str(round(precision_value,5))+', Recall='+str(round(recall_value,5))+', NDCG='+str(round(ndcg_value,5))+', MRR='+str(round(mrr_value,5))+', MAP='+str(round(map_value,5)))\n",
    "print('------------------------------------------')\n",
    "print('Performance of content-based recommender for content type = full and aggregation method = weighted_avg:')\n",
    "precision_value, recall_value, ndcg_value, mrr_value, map_value = evaluate_rating_prediction(train_data, test_data, 'full', 'weighted_avg')\n",
    "print('Precision='+str(round(precision_value,5))+', Recall='+str(round(recall_value,5))+', NDCG='+str(round(ndcg_value,5))+', MRR='+str(round(mrr_value,5))+', MAP='+str(round(map_value,5)))\n",
    "print('------------------------------------------')\n",
    "print('Performance of content-based recommender for content type = title+genres and aggregation method = avg_pos:')\n",
    "precision_value, recall_value, ndcg_value, mrr_value, map_value = evaluate_rating_prediction(train_data, test_data, 'title_genres', 'avg_pos')\n",
    "print('Precision='+str(round(precision_value,5))+', Recall='+str(round(recall_value,5))+', NDCG='+str(round(ndcg_value,5))+', MRR='+str(round(mrr_value,5))+', MAP='+str(round(map_value,5)))\n",
    "print('------------------------------------------')\n",
    "print('Performance of content-based recommender for content type = description and aggregation method = avg_pos:')\n",
    "precision_value, recall_value, ndcg_value, mrr_value, map_value = evaluate_rating_prediction(train_data, test_data, 'description', 'avg_pos')\n",
    "print('Precision='+str(round(precision_value,5))+', Recall='+str(round(recall_value,5))+', NDCG='+str(round(ndcg_value,5))+', MRR='+str(round(mrr_value,5))+', MAP='+str(round(map_value,5)))\n",
    "print('------------------------------------------')\n",
    "print('Performance of content-based recommender for content type = full and aggregation method = avg_pos:')\n",
    "precision_value, recall_value, ndcg_value, mrr_value, map_value = evaluate_rating_prediction(train_data, test_data, 'full', 'avg_pos')\n",
    "print('Precision='+str(round(precision_value,5))+', Recall='+str(round(recall_value,5))+', NDCG='+str(round(ndcg_value,5))+', MRR='+str(round(mrr_value,5))+', MAP='+str(round(map_value,5)))\n",
    "print('------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f86967-be28-45dc-ac3b-c2a8b605191a",
   "metadata": {},
   "source": [
    "Discuss your observations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25401334-3090-4c0d-b291-e98571bf2fa4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5f16da6a-89fd-4168-90f0-af61d082b6cf",
   "metadata": {},
   "source": [
    "# Extended experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104ec9bf-6192-4a56-bb37-c0a20837c678",
   "metadata": {},
   "source": [
    "### Question 14: Implement a baseline for rating prediction task that returns average rating of target item as the predicted rating. \n",
    "\n",
    "For example, for a user u and item i, the prediction is the average of ratings given to i in training data.\n",
    "\n",
    "#### Evaluate the performance of this baseline in terms of MAE, MSE, RMSE, and compare it with the results in Question 12. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "27a46912-a5a0-4230-b378-f83cf50b9fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_item_avg_baseline(train_data, test_data):\n",
    "    # Hint: no normalization is needed.\n",
    "    \n",
    "    actual_ratings, pred_ratings = [], []\n",
    "    \n",
    "    ############# Your code here ############\n",
    "    \n",
    "    #########################################\n",
    "\n",
    "    mae_value, mse_value, rmse_value = 0.0, 0.0, 0.0\n",
    "\n",
    "    # compute the metrics: MAE, MSE, RMSE \n",
    "    ############# Your code here ############\n",
    "    \n",
    "    #########################################\n",
    "\n",
    "    return mae_value, mse_value, rmse_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543bfc22-3a41-414b-aab4-805bc7fb6371",
   "metadata": {},
   "source": [
    "Discuss your observations:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf449a68-b797-47bd-9687-58b20e3e9c4a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "96416e08-a74a-4cfd-aa7d-1cd03ff2c178",
   "metadata": {},
   "source": [
    "### Question 15: Implement the following baselines for ranking task:\n",
    "\n",
    "**Random:** Randomly recommend 10 unseen items (items not interacted by the target user in training data) to each user\n",
    "\n",
    "**Popular:** Recommend 10 most popular items that are not yet interacted by the target user. Most popular items are the ones that are rated by majority of users in the training data.\n",
    "\n",
    "#### Evaluate the performance of these baselines in terms of Precision, Recall, NDCG, MRR, and MAP and compare them with the results in Question 13. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a15041-ccec-4a04-8c2e-b3e414becf73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_random_baseline(train_data, test_data):\n",
    "    # Hint: no normalization is needed.\n",
    "    \n",
    "    ground_truth, rec_list = [], []\n",
    "    \n",
    "    ############# Your code here ############\n",
    "    \n",
    "    #########################################\n",
    "\n",
    "    precision_value, recall_value, ndcg_value, mrr_value, map_value = 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "\n",
    "    # compute the metrics: Precision, Recall, NDCG, MRR, MAP \n",
    "    ############# Your code here ############\n",
    "    \n",
    "    #########################################\n",
    "\n",
    "    return precision_value, recall_value, ndcg_value, mrr_value, map_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "74b0f778-3e78-4c53-a2bf-31b70616c6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_popular_baseline(train_data, test_data):\n",
    "    # Hint: no normalization is needed.\n",
    "    \n",
    "    ground_truth, rec_list = [], []\n",
    "    \n",
    "    ############# Your code here ############\n",
    "    \n",
    "    #########################################\n",
    "\n",
    "    precision_value, recall_value, ndcg_value, mrr_value, map_value = 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "\n",
    "    # compute the metrics: Precision, Recall, NDCG, MRR, MAP\n",
    "    ############# Your code here ############\n",
    "    \n",
    "    #########################################\n",
    "\n",
    "    return precision_value, recall_value, ndcg_value, mrr_value, map_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af29f4e5-fe44-4208-8793-b6f53a82a81f",
   "metadata": {},
   "source": [
    "Discuss your observations:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c628d680-b32c-4ed2-8de5-57210f453f75",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
